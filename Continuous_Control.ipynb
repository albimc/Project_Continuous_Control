{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy\n",
    "import inspect\n",
    "import time\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# from ddpg_agent import Agent\n",
    "# from ddpg_model import Actor, Critic\n",
    "# from ddpg_interact import Interact\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Agents\n",
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "We are using a 20 Agents environment\n",
    "\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Reacher_Linux_NoVis_20/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReacherBrain\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "print (brain_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726624e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.14099999684840442\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters 1e-3\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor 1e-4\n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay 0\n",
    "TIMESTEPS_WAIT = 1     # how many timesteps to wait forlearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, num_agents, seed=1, timestamp=1):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise((num_agents, action_size), seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.timestamp = timestamp\n",
    "\n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience / reward\n",
    "        # self.memory.add(state, action, reward, next_state, done)\n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "        self.timestamp += 1\n",
    "\n",
    "        # Learn, if enough samples are available in memory\n",
    "        oktolearn = (self.timestamp % TIMESTEPS_WAIT) == 0\n",
    "        okbatchsize = len(self.memory) > BATCH_SIZE\n",
    "        # print(oktolearn, okbatchsize)\n",
    "        if (okbatchsize & oktolearn):\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, add_noise=True, eps=1.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        if add_noise:\n",
    "            action += self.noise.sample(epsilon=eps)\n",
    "        # action = (action + 1.0) / 2.0\n",
    "        # return np.clip(action, 0, 1)\n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic_local.parameters(), 1)  # Grad clipping\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        # print('Noise Process: mean {}, drift {}, stdev {} '.format(mu, theta, sigma))\n",
    "        self.size = size\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self, epsilon=1.):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        # dx = self.theta * (self.mu - x) + self.sigma * epsilon * np.array([np.random.randn() for i in range(len(x))])\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * epsilon * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Actor - Critic Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        # self.fc2_drop = nn.Dropout(p=0.05)          # Dropout\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.leaky_relu(self.fc1(state))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        # x = self.fc2_drop(x)                        # Dropout   \n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        return F.tanh(x)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fcs1_units=128, fc2_units=128):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fcs1 = nn.Linear(state_size, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units + action_size, fc2_units)\n",
    "        # self.fc2_drop = nn.Dropout(p=0.05)                     # Dropout\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        xs = F.leaky_relu(self.fcs1(state))\n",
    "        x = torch.cat((xs, action), dim=1)\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        # x = self.fc2_drop(x)                                     # Dropout\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Interact(env, agent, brain_name, num_agents, train_mode=True, add_noise=True,\n",
    "             n_episodes=1000, max_t=400, eps_start=1.0, eps_end=0.1, eps_decay=0.999, n_window=40, tgt_score=30):\n",
    "    \"\"\"\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    print('\\rRunning for {} episodes with target score of {}'.format(n_episodes, tgt_score))\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('\\rUsing {}'.format(device))\n",
    "    start = time.time()\n",
    "\n",
    "    # Initialise\n",
    "    scores = []                                             # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=n_window)                  # last 100 scores\n",
    "    eps = eps_start                                         # initialize epsilon\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name]    # reset the environment\n",
    "        states = env_info.vector_observations                    # get the current state\n",
    "        score = np.zeros(num_agents)\n",
    "        for t in range(max_t):\n",
    "            # Agent Selects Action #\n",
    "            actions = agent.act(states, add_noise=add_noise, eps=eps)     # Agent action\n",
    "            # Agent Performs Action #\n",
    "            env_info = env.step(actions)[brain_name]                # Environment reacts\n",
    "            next_states = env_info.vector_observations           # get the next state\n",
    "            rewards = env_info.rewards                           # get the reward\n",
    "            dones = env_info.local_done                          # see if episode has finished\n",
    "            # Agent Observes State #\n",
    "            agent.step(states, actions, rewards, next_states, dones)    # Agent observes new state\n",
    "            states = next_states                                     # roll over the state to next time step\n",
    "            score += rewards                                        # update the score\n",
    "            if any(dones):\n",
    "                break\n",
    "        scores_window.append(np.mean(score))        # save most recent score\n",
    "        scores.append(np.mean(score))               # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps)           # decrease epsilon\n",
    "        # Periodic Check\n",
    "        if i_episode % n_window == 0:\n",
    "            end = time.time()\n",
    "            print('\\nElapsed time {:.1f}'.format((end - start)/60), 'Steps {}'.format(i_episode*max_t), 'Agent Steps {}'.format(agent.timestamp), end=\"\")\n",
    "            print('\\nEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "            print('\\nEpisode {}\\tLast Action:'.format(i_episode), actions[0], end=\"\")\n",
    "            print('\\nEpisode {}\\tLast Epsilon: {:.2f}'.format(i_episode, eps), end=\"\")\n",
    "            torch.save(agent.actor_local.state_dict(), './Data/checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), './Data/checkpoint_critic.pth')\n",
    "        if np.mean(scores_window) >= tgt_score:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-n_window, np.mean(scores_window)))\n",
    "            break\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise Process: mean 0.0, drift 0.15, stdev 0.2 \n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=state_size, action_size=action_size, num_agents=num_agents, seed=1, timestamp=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUFFER_SIZE 100000, BATCH_SIZE 128, GAMMA 0.99, TAU 0.001,\n",
      "\t\t LR_ACTOR 0.001, LR_CRITIC 0.001, WEIGHT_DECAY 0, TIMESTEPS_WAIT 1\n",
      "Agent: (state_size, action_size, num_agents, seed=1, timestamp=1)\n",
      "Noise: (size, seed, mu=0.0, theta=0.15, sigma=0.2)\n"
     ]
    }
   ],
   "source": [
    "print('BUFFER_SIZE {}, BATCH_SIZE {}, GAMMA {}, TAU {},\\n\\t\\t LR_ACTOR {}, LR_CRITIC {}, WEIGHT_DECAY {}, TIMESTEPS_WAIT {}'.format(BUFFER_SIZE, BATCH_SIZE, GAMMA, TAU, LR_ACTOR, LR_CRITIC, WEIGHT_DECAY, TIMESTEPS_WAIT))\n",
    "print('Agent:',inspect.signature(agent.__init__))\n",
    "print('Noise:',inspect.signature(agent.noise.__init__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for 1000 episodes with target score of 40\n",
      "Using cuda:0\n",
      "\n",
      "Elapsed time 2.4 Steps 10000 Agent Steps 10001\n",
      "Episode 10\tAverage Score: 0.78\n",
      "Episode 10\tLast Action: [-1.          0.7341786   0.6690035   0.72353077]\n",
      "Episode 10\tLast Epsilon: 0.99\n",
      "Elapsed time 4.7 Steps 20000 Agent Steps 20001\n",
      "Episode 20\tAverage Score: 2.49\n",
      "Episode 20\tLast Action: [-1.          1.          0.83675295  0.80358607]\n",
      "Episode 20\tLast Epsilon: 0.98\n",
      "Elapsed time 7.1 Steps 30000 Agent Steps 30001\n",
      "Episode 30\tAverage Score: 7.93\n",
      "Episode 30\tLast Action: [ 1.          0.9697141   0.6633419  -0.32955512]\n",
      "Episode 30\tLast Epsilon: 0.97\n",
      "Elapsed time 9.6 Steps 40000 Agent Steps 40001\n",
      "Episode 40\tAverage Score: 13.85\n",
      "Episode 40\tLast Action: [ 0.19294363 -0.65493757  1.         -0.787564  ]\n",
      "Episode 40\tLast Epsilon: 0.96\n",
      "Elapsed time 12.0 Steps 50000 Agent Steps 50001\n",
      "Episode 50\tAverage Score: 22.93\n",
      "Episode 50\tLast Action: [-1.          0.6278692   0.14977092 -0.5603108 ]\n",
      "Episode 50\tLast Epsilon: 0.95\n",
      "Elapsed time 14.4 Steps 60000 Agent Steps 60001\n",
      "Episode 60\tAverage Score: 34.79\n",
      "Episode 60\tLast Action: [ 0.3794121   0.4897575   0.9159375  -0.12935899]\n",
      "Episode 60\tLast Epsilon: 0.94\n",
      "Elapsed time 16.8 Steps 70000 Agent Steps 70001\n",
      "Episode 70\tAverage Score: 37.83\n",
      "Episode 70\tLast Action: [-0.35078698  1.          0.9938024  -0.51606315]\n",
      "Episode 70\tLast Epsilon: 0.93\n",
      "Elapsed time 19.2 Steps 80000 Agent Steps 80001\n",
      "Episode 80\tAverage Score: 36.99\n",
      "Episode 80\tLast Action: [-0.46862832  0.6316309   0.2359671  -1.        ]\n",
      "Episode 80\tLast Epsilon: 0.92\n",
      "Elapsed time 21.6 Steps 90000 Agent Steps 90001\n",
      "Episode 90\tAverage Score: 37.55\n",
      "Episode 90\tLast Action: [ 0.01602849 -0.68389153  1.          0.3745431 ]\n",
      "Episode 90\tLast Epsilon: 0.91\n",
      "Elapsed time 24.0 Steps 100000 Agent Steps 100001\n",
      "Episode 100\tAverage Score: 37.80\n",
      "Episode 100\tLast Action: [-1.          0.07383849 -0.2726919   0.90351284]\n",
      "Episode 100\tLast Epsilon: 0.90\n",
      "Elapsed time 26.5 Steps 110000 Agent Steps 110001\n",
      "Episode 110\tAverage Score: 36.22\n",
      "Episode 110\tLast Action: [-0.10948282 -0.566039    0.01178228  0.7240611 ]\n",
      "Episode 110\tLast Epsilon: 0.90\n",
      "Elapsed time 28.9 Steps 120000 Agent Steps 120001\n",
      "Episode 120\tAverage Score: 37.78\n",
      "Episode 120\tLast Action: [-0.4461939   0.28905338 -0.4227261  -0.03287707]\n",
      "Episode 120\tLast Epsilon: 0.89\n",
      "Elapsed time 31.3 Steps 130000 Agent Steps 130001\n",
      "Episode 130\tAverage Score: 38.19\n",
      "Episode 130\tLast Action: [0.81120145 1.         0.6141207  0.10656369]\n",
      "Episode 130\tLast Epsilon: 0.88\n",
      "Elapsed time 33.7 Steps 140000 Agent Steps 140001\n",
      "Episode 140\tAverage Score: 37.22\n",
      "Episode 140\tLast Action: [-0.41613644 -0.11619788 -0.80587375 -1.        ]\n",
      "Episode 140\tLast Epsilon: 0.87\n",
      "Elapsed time 36.1 Steps 150000 Agent Steps 150001\n",
      "Episode 150\tAverage Score: 37.31\n",
      "Episode 150\tLast Action: [ 0.5326653   0.23297258 -0.04165293  1.        ]\n",
      "Episode 150\tLast Epsilon: 0.86\n",
      "Elapsed time 38.5 Steps 160000 Agent Steps 160001\n",
      "Episode 160\tAverage Score: 37.77\n",
      "Episode 160\tLast Action: [ 1.        -0.7978485  1.         0.430272 ]\n",
      "Episode 160\tLast Epsilon: 0.85\n",
      "Elapsed time 40.9 Steps 170000 Agent Steps 170001\n",
      "Episode 170\tAverage Score: 37.21\n",
      "Episode 170\tLast Action: [ 0.7816455  -0.7730313   0.00576601  0.90852374]\n",
      "Episode 170\tLast Epsilon: 0.84\n",
      "Elapsed time 43.3 Steps 180000 Agent Steps 180001\n",
      "Episode 180\tAverage Score: 37.27\n",
      "Episode 180\tLast Action: [ 1.         -1.          0.87779665  0.4406027 ]\n",
      "Episode 180\tLast Epsilon: 0.84\n",
      "Elapsed time 45.7 Steps 190000 Agent Steps 190001\n",
      "Episode 190\tAverage Score: 37.98\n",
      "Episode 190\tLast Action: [-0.47970203 -0.6511409  -0.6621866   0.06575959]\n",
      "Episode 190\tLast Epsilon: 0.83\n",
      "Elapsed time 48.2 Steps 200000 Agent Steps 200001\n",
      "Episode 200\tAverage Score: 37.63\n",
      "Episode 200\tLast Action: [0.8730106 1.        1.        1.       ]\n",
      "Episode 200\tLast Epsilon: 0.82\n",
      "Elapsed time 50.6 Steps 210000 Agent Steps 210001\n",
      "Episode 210\tAverage Score: 37.62\n",
      "Episode 210\tLast Action: [-1.          1.         -0.76716685  0.6393336 ]\n",
      "Episode 210\tLast Epsilon: 0.81\n",
      "Elapsed time 53.0 Steps 220000 Agent Steps 220001\n",
      "Episode 220\tAverage Score: 37.92\n",
      "Episode 220\tLast Action: [-0.60580486 -0.8617664  -0.11139454 -0.1478558 ]\n",
      "Episode 220\tLast Epsilon: 0.80\n",
      "Elapsed time 55.4 Steps 230000 Agent Steps 230001\n",
      "Episode 230\tAverage Score: 37.60\n",
      "Episode 230\tLast Action: [ 0.9845275 -0.8518662  1.         0.7555583]\n",
      "Episode 230\tLast Epsilon: 0.79\n",
      "Elapsed time 57.8 Steps 240000 Agent Steps 240001\n",
      "Episode 240\tAverage Score: 37.09\n",
      "Episode 240\tLast Action: [-0.38048854  0.67361313 -0.5269411  -0.21075419]\n",
      "Episode 240\tLast Epsilon: 0.79\n",
      "Elapsed time 60.2 Steps 250000 Agent Steps 250001\n",
      "Episode 250\tAverage Score: 36.66\n",
      "Episode 250\tLast Action: [-0.82705176  0.01308843 -0.30128902 -0.10969903]\n",
      "Episode 250\tLast Epsilon: 0.78\n",
      "Elapsed time 62.6 Steps 260000 Agent Steps 260001\n",
      "Episode 260\tAverage Score: 37.33\n",
      "Episode 260\tLast Action: [ 0.4883928  -0.54333335 -0.21248409  0.1665002 ]\n",
      "Episode 260\tLast Epsilon: 0.77\n",
      "Elapsed time 65.0 Steps 270000 Agent Steps 270001\n",
      "Episode 270\tAverage Score: 38.04\n",
      "Episode 270\tLast Action: [ 0.59899914 -0.59709823  0.78296375 -0.3858154 ]\n",
      "Episode 270\tLast Epsilon: 0.76\n",
      "Elapsed time 67.5 Steps 280000 Agent Steps 280001\n",
      "Episode 280\tAverage Score: 38.13\n",
      "Episode 280\tLast Action: [ 1.          0.28192535  0.5454273  -0.15813781]\n",
      "Episode 280\tLast Epsilon: 0.76\n",
      "Elapsed time 69.9 Steps 290000 Agent Steps 290001\n",
      "Episode 290\tAverage Score: 37.90\n",
      "Episode 290\tLast Action: [ 0.8621543 -0.9385353  0.8862539  1.       ]\n",
      "Episode 290\tLast Epsilon: 0.75\n",
      "Elapsed time 72.4 Steps 300000 Agent Steps 300001\n",
      "Episode 300\tAverage Score: 37.88\n",
      "Episode 300\tLast Action: [-0.13549916 -1.         -0.19613606  0.03015271]\n",
      "Episode 300\tLast Epsilon: 0.74\n",
      "Elapsed time 74.9 Steps 310000 Agent Steps 310001\n",
      "Episode 310\tAverage Score: 37.18\n",
      "Episode 310\tLast Action: [-0.22527422 -0.8130373   0.93840426 -0.01096179]\n",
      "Episode 310\tLast Epsilon: 0.73\n",
      "Elapsed time 77.3 Steps 320000 Agent Steps 320001\n",
      "Episode 320\tAverage Score: 36.14\n",
      "Episode 320\tLast Action: [-0.17682402 -0.9203744  -0.93888676 -0.04884662]\n",
      "Episode 320\tLast Epsilon: 0.73\n",
      "Elapsed time 79.8 Steps 330000 Agent Steps 330001\n",
      "Episode 330\tAverage Score: 38.31\n",
      "Episode 330\tLast Action: [-0.7149664   0.06345907  0.754778   -0.20433997]\n",
      "Episode 330\tLast Epsilon: 0.72\n",
      "Elapsed time 82.2 Steps 340000 Agent Steps 340001\n",
      "Episode 340\tAverage Score: 37.88\n",
      "Episode 340\tLast Action: [ 1.        -1.         0.8727218 -0.5789941]\n",
      "Episode 340\tLast Epsilon: 0.71\n",
      "Elapsed time 84.8 Steps 350000 Agent Steps 350001\n",
      "Episode 350\tAverage Score: 37.34\n",
      "Episode 350\tLast Action: [-0.68258804 -0.0979932   0.19662666  1.        ]\n",
      "Episode 350\tLast Epsilon: 0.70\n",
      "Elapsed time 87.2 Steps 360000 Agent Steps 360001\n",
      "Episode 360\tAverage Score: 35.82\n",
      "Episode 360\tLast Action: [-0.79222506 -0.20040764 -0.0633934   0.10541317]\n",
      "Episode 360\tLast Epsilon: 0.70\n",
      "Elapsed time 89.6 Steps 370000 Agent Steps 370001\n",
      "Episode 370\tAverage Score: 36.57\n",
      "Episode 370\tLast Action: [-0.8861207  -0.55195224 -0.6658731   1.        ]\n",
      "Episode 370\tLast Epsilon: 0.69\n",
      "Elapsed time 92.0 Steps 380000 Agent Steps 380001\n",
      "Episode 380\tAverage Score: 37.38\n",
      "Episode 380\tLast Action: [ 0.19914393  0.6201633  -0.4166552  -0.02280471]\n",
      "Episode 380\tLast Epsilon: 0.68\n",
      "Elapsed time 94.4 Steps 390000 Agent Steps 390001\n",
      "Episode 390\tAverage Score: 37.34\n",
      "Episode 390\tLast Action: [ 0.2921378  -1.          0.6900748  -0.20945542]\n",
      "Episode 390\tLast Epsilon: 0.68\n",
      "Elapsed time 96.8 Steps 400000 Agent Steps 400001\n",
      "Episode 400\tAverage Score: 37.88\n",
      "Episode 400\tLast Action: [-0.70890003  1.         -0.89963895 -0.58819103]\n",
      "Episode 400\tLast Epsilon: 0.67\n",
      "Elapsed time 99.2 Steps 410000 Agent Steps 410001\n",
      "Episode 410\tAverage Score: 37.68\n",
      "Episode 410\tLast Action: [-0.6649386 -1.         1.        -0.4655926]\n",
      "Episode 410\tLast Epsilon: 0.66\n",
      "Elapsed time 101.7 Steps 420000 Agent Steps 420001\n",
      "Episode 420\tAverage Score: 37.75\n",
      "Episode 420\tLast Action: [ 1.         -0.53402257  0.16301471  0.29241282]\n",
      "Episode 420\tLast Epsilon: 0.66\n",
      "Elapsed time 104.1 Steps 430000 Agent Steps 430001\n",
      "Episode 430\tAverage Score: 37.66\n",
      "Episode 430\tLast Action: [-1.          0.5227076  -0.03692675  0.71527916]\n",
      "Episode 430\tLast Epsilon: 0.65\n",
      "Elapsed time 106.5 Steps 440000 Agent Steps 440001\n",
      "Episode 440\tAverage Score: 37.32\n",
      "Episode 440\tLast Action: [-1.         -1.         -0.80612206  0.6316319 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 440\tLast Epsilon: 0.64\n",
      "Elapsed time 108.9 Steps 450000 Agent Steps 450001\n",
      "Episode 450\tAverage Score: 36.83\n",
      "Episode 450\tLast Action: [ 0.2083184   0.96528697 -0.6409631   0.21981227]\n",
      "Episode 450\tLast Epsilon: 0.64\n",
      "Elapsed time 111.4 Steps 460000 Agent Steps 460001\n",
      "Episode 460\tAverage Score: 38.37\n",
      "Episode 460\tLast Action: [ 0.6905015  -0.18032442  0.9886065  -1.        ]\n",
      "Episode 460\tLast Epsilon: 0.63\n",
      "Elapsed time 113.7 Steps 470000 Agent Steps 470001\n",
      "Episode 470\tAverage Score: 37.93\n",
      "Episode 470\tLast Action: [ 1.          0.6724396  -0.07035775 -0.8851159 ]\n",
      "Episode 470\tLast Epsilon: 0.62\n",
      "Elapsed time 115.9 Steps 480000 Agent Steps 480001\n",
      "Episode 480\tAverage Score: 37.15\n",
      "Episode 480\tLast Action: [-0.8140661  -1.         -0.71783495 -0.54585093]\n",
      "Episode 480\tLast Epsilon: 0.62\n",
      "Elapsed time 118.1 Steps 490000 Agent Steps 490001\n",
      "Episode 490\tAverage Score: 36.71\n",
      "Episode 490\tLast Action: [ 0.7046014 -0.978834  -0.8926977 -0.8683122]\n",
      "Episode 490\tLast Epsilon: 0.61\n",
      "Elapsed time 120.3 Steps 500000 Agent Steps 500001\n",
      "Episode 500\tAverage Score: 37.83\n",
      "Episode 500\tLast Action: [ 0.61116225 -0.4278955  -0.12665749  0.6802074 ]\n",
      "Episode 500\tLast Epsilon: 0.61\n",
      "Elapsed time 122.7 Steps 510000 Agent Steps 510001\n",
      "Episode 510\tAverage Score: 37.48\n",
      "Episode 510\tLast Action: [-0.9185649  -0.99796015 -0.33321583 -0.18373774]\n",
      "Episode 510\tLast Epsilon: 0.60\n",
      "Elapsed time 125.0 Steps 520000 Agent Steps 520001\n",
      "Episode 520\tAverage Score: 34.41\n",
      "Episode 520\tLast Action: [ 0.83813775 -0.42219493 -0.5383229  -0.5787088 ]\n",
      "Episode 520\tLast Epsilon: 0.59\n",
      "Elapsed time 127.3 Steps 530000 Agent Steps 530001\n",
      "Episode 530\tAverage Score: 38.16\n",
      "Episode 530\tLast Action: [ 0.9395906   1.         -0.13794643 -0.30340922]\n",
      "Episode 530\tLast Epsilon: 0.59\n",
      "Elapsed time 129.6 Steps 540000 Agent Steps 540001\n",
      "Episode 540\tAverage Score: 37.85\n",
      "Episode 540\tLast Action: [-1.          0.5333636  -0.09831858 -0.4988868 ]\n",
      "Episode 540\tLast Epsilon: 0.58\n",
      "Elapsed time 131.9 Steps 550000 Agent Steps 550001\n",
      "Episode 550\tAverage Score: 37.21\n",
      "Episode 550\tLast Action: [-0.10452816 -0.1652894  -0.16223097  0.9913966 ]\n",
      "Episode 550\tLast Epsilon: 0.58\n",
      "Elapsed time 134.1 Steps 560000 Agent Steps 560001\n",
      "Episode 560\tAverage Score: 37.10\n",
      "Episode 560\tLast Action: [-0.5477259  -0.6555376  -0.30646238 -0.1375033 ]\n",
      "Episode 560\tLast Epsilon: 0.57\n",
      "Elapsed time 136.4 Steps 570000 Agent Steps 570001\n",
      "Episode 570\tAverage Score: 38.05\n",
      "Episode 570\tLast Action: [ 0.9483327   1.         -0.11997532  1.        ]\n",
      "Episode 570\tLast Epsilon: 0.57\n",
      "Elapsed time 138.8 Steps 580000 Agent Steps 580001\n",
      "Episode 580\tAverage Score: 35.89\n",
      "Episode 580\tLast Action: [-1.         1.        -0.2114043 -0.7233247]\n",
      "Episode 580\tLast Epsilon: 0.56\n",
      "Elapsed time 141.2 Steps 590000 Agent Steps 590001\n",
      "Episode 590\tAverage Score: 36.47\n",
      "Episode 590\tLast Action: [ 1.         -0.41390726 -0.3828009   0.02671474]\n",
      "Episode 590\tLast Epsilon: 0.55\n",
      "Elapsed time 143.5 Steps 600000 Agent Steps 600001\n",
      "Episode 600\tAverage Score: 37.58\n",
      "Episode 600\tLast Action: [ 0.77209115 -0.7928168   1.         -0.10216431]\n",
      "Episode 600\tLast Epsilon: 0.55\n",
      "Elapsed time 145.8 Steps 610000 Agent Steps 610001\n",
      "Episode 610\tAverage Score: 38.23\n",
      "Episode 610\tLast Action: [-1.         -1.          0.04866297 -0.45478192]\n",
      "Episode 610\tLast Epsilon: 0.54\n",
      "Elapsed time 148.1 Steps 620000 Agent Steps 620001\n",
      "Episode 620\tAverage Score: 36.30\n",
      "Episode 620\tLast Action: [ 0.89919585  1.         -0.56939936 -1.        ]\n",
      "Episode 620\tLast Epsilon: 0.54\n",
      "Elapsed time 150.4 Steps 630000 Agent Steps 630001\n",
      "Episode 630\tAverage Score: 33.94\n",
      "Episode 630\tLast Action: [-1.          0.91032714 -0.77807003  0.6513916 ]\n",
      "Episode 630\tLast Epsilon: 0.53\n",
      "Elapsed time 152.7 Steps 640000 Agent Steps 640001\n",
      "Episode 640\tAverage Score: 35.47\n",
      "Episode 640\tLast Action: [ 0.7712373  0.9485667 -0.6873712  0.9991767]\n",
      "Episode 640\tLast Epsilon: 0.53\n",
      "Elapsed time 155.0 Steps 650000 Agent Steps 650001\n",
      "Episode 650\tAverage Score: 35.91\n",
      "Episode 650\tLast Action: [-0.45036575 -0.96334136 -0.26866722 -0.92623776]\n",
      "Episode 650\tLast Epsilon: 0.52\n",
      "Elapsed time 157.4 Steps 660000 Agent Steps 660001\n",
      "Episode 660\tAverage Score: 37.43\n",
      "Episode 660\tLast Action: [-1.          0.25053665 -0.34665433  0.9717907 ]\n",
      "Episode 660\tLast Epsilon: 0.52\n",
      "Elapsed time 159.7 Steps 670000 Agent Steps 670001\n",
      "Episode 670\tAverage Score: 37.14\n",
      "Episode 670\tLast Action: [ 0.9733664  -1.          1.          0.08686918]\n",
      "Episode 670\tLast Epsilon: 0.51\n",
      "Elapsed time 162.0 Steps 680000 Agent Steps 680001\n",
      "Episode 680\tAverage Score: 36.86\n",
      "Episode 680\tLast Action: [-0.8939625  -0.7688657  -0.06404626  0.05316348]\n",
      "Episode 680\tLast Epsilon: 0.51\n",
      "Elapsed time 164.3 Steps 690000 Agent Steps 690001\n",
      "Episode 690\tAverage Score: 36.82\n",
      "Episode 690\tLast Action: [ 0.24039938  1.          0.87728953 -0.50919074]\n",
      "Episode 690\tLast Epsilon: 0.50\n",
      "Elapsed time 166.7 Steps 700000 Agent Steps 700001\n",
      "Episode 700\tAverage Score: 34.82\n",
      "Episode 700\tLast Action: [-0.9644691  -0.8214611   0.5104299  -0.21975882]\n",
      "Episode 700\tLast Epsilon: 0.50\n",
      "Elapsed time 169.1 Steps 710000 Agent Steps 710001\n",
      "Episode 710\tAverage Score: 34.09\n",
      "Episode 710\tLast Action: [ 1.          0.7949148  -0.09144326 -0.05823967]\n",
      "Episode 710\tLast Epsilon: 0.49\n",
      "Elapsed time 171.4 Steps 720000 Agent Steps 720001\n",
      "Episode 720\tAverage Score: 29.01\n",
      "Episode 720\tLast Action: [-0.2901272  -0.85858244 -0.7353116   0.93314254]\n",
      "Episode 720\tLast Epsilon: 0.49\n",
      "Elapsed time 173.8 Steps 730000 Agent Steps 730001\n",
      "Episode 730\tAverage Score: 35.58\n",
      "Episode 730\tLast Action: [-1.         -0.26412812 -0.08081475 -0.02800058]\n",
      "Episode 730\tLast Epsilon: 0.48\n",
      "Elapsed time 176.0 Steps 740000 Agent Steps 740001\n",
      "Episode 740\tAverage Score: 34.78\n",
      "Episode 740\tLast Action: [ 1.         -1.         -0.12568086  1.        ]\n",
      "Episode 740\tLast Epsilon: 0.48\n",
      "Elapsed time 178.6 Steps 750000 Agent Steps 750001\n",
      "Episode 750\tAverage Score: 35.21\n",
      "Episode 750\tLast Action: [ 0.901745    0.70593435  0.19512087 -0.48387888]\n",
      "Episode 750\tLast Epsilon: 0.47\n",
      "Elapsed time 181.1 Steps 760000 Agent Steps 760001\n",
      "Episode 760\tAverage Score: 37.28\n",
      "Episode 760\tLast Action: [-0.6315135 -0.6771676 -0.4168761 -0.3763373]\n",
      "Episode 760\tLast Epsilon: 0.47\n",
      "Elapsed time 183.6 Steps 770000 Agent Steps 770001\n",
      "Episode 770\tAverage Score: 37.61\n",
      "Episode 770\tLast Action: [-0.77012795  0.03138635 -0.33407596  0.9261217 ]\n",
      "Episode 770\tLast Epsilon: 0.46\n",
      "Elapsed time 186.1 Steps 780000 Agent Steps 780001\n",
      "Episode 780\tAverage Score: 37.22\n",
      "Episode 780\tLast Action: [ 1.          0.7297956  -0.20395574  1.        ]\n",
      "Episode 780\tLast Epsilon: 0.46\n",
      "Elapsed time 188.7 Steps 790000 Agent Steps 790001\n",
      "Episode 790\tAverage Score: 35.18\n",
      "Episode 790\tLast Action: [-1.         -1.          1.          0.93926734]\n",
      "Episode 790\tLast Epsilon: 0.45\n",
      "Elapsed time 191.2 Steps 800000 Agent Steps 800001\n",
      "Episode 800\tAverage Score: 35.79\n",
      "Episode 800\tLast Action: [ 1.         -0.76574314  0.13081817 -1.        ]\n",
      "Episode 800\tLast Epsilon: 0.45\n",
      "Elapsed time 193.8 Steps 810000 Agent Steps 810001\n",
      "Episode 810\tAverage Score: 36.82\n",
      "Episode 810\tLast Action: [-0.5316841 -1.        -0.4729256 -0.6318605]\n",
      "Episode 810\tLast Epsilon: 0.44\n",
      "Elapsed time 196.3 Steps 820000 Agent Steps 820001\n",
      "Episode 820\tAverage Score: 37.21\n",
      "Episode 820\tLast Action: [-0.01336735 -0.6132321   0.02323217 -0.10912913]\n",
      "Episode 820\tLast Epsilon: 0.44\n",
      "Elapsed time 198.8 Steps 830000 Agent Steps 830001\n",
      "Episode 830\tAverage Score: 36.74\n",
      "Episode 830\tLast Action: [ 0.853452   -0.6755749  -0.13553238  0.13549341]\n",
      "Episode 830\tLast Epsilon: 0.44\n",
      "Elapsed time 201.4 Steps 840000 Agent Steps 840001\n",
      "Episode 840\tAverage Score: 37.74\n",
      "Episode 840\tLast Action: [ 0.44074982  0.8948106  -0.08822614  1.        ]\n",
      "Episode 840\tLast Epsilon: 0.43\n",
      "Elapsed time 203.9 Steps 850000 Agent Steps 850001\n",
      "Episode 850\tAverage Score: 35.01\n",
      "Episode 850\tLast Action: [ 0.93509996  0.98447514 -0.45286137 -0.87392545]\n",
      "Episode 850\tLast Epsilon: 0.43\n",
      "Elapsed time 206.4 Steps 860000 Agent Steps 860001\n",
      "Episode 860\tAverage Score: 34.64\n",
      "Episode 860\tLast Action: [-0.4056112   0.82516736 -0.02720319 -0.20522128]\n",
      "Episode 860\tLast Epsilon: 0.42\n",
      "Elapsed time 209.0 Steps 870000 Agent Steps 870001\n",
      "Episode 870\tAverage Score: 32.17\n",
      "Episode 870\tLast Action: [-0.9791221   1.         -0.06103073 -0.14228433]\n",
      "Episode 870\tLast Epsilon: 0.42\n",
      "Elapsed time 211.5 Steps 880000 Agent Steps 880001\n",
      "Episode 880\tAverage Score: 36.57\n",
      "Episode 880\tLast Action: [ 0.793162   -0.86793345  0.96297467 -0.06799557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 880\tLast Epsilon: 0.41\n",
      "Elapsed time 214.1 Steps 890000 Agent Steps 890001\n",
      "Episode 890\tAverage Score: 37.43\n",
      "Episode 890\tLast Action: [ 1.         -0.81832665 -0.21065664 -0.02857242]\n",
      "Episode 890\tLast Epsilon: 0.41\n",
      "Elapsed time 216.6 Steps 900000 Agent Steps 900001\n",
      "Episode 900\tAverage Score: 38.13\n",
      "Episode 900\tLast Action: [-0.7723344  -0.57772267 -0.5445637   0.00499638]\n",
      "Episode 900\tLast Epsilon: 0.41\n",
      "Elapsed time 219.1 Steps 910000 Agent Steps 910001\n",
      "Episode 910\tAverage Score: 36.59\n",
      "Episode 910\tLast Action: [ 1.         -1.         -0.9963985  -0.39946988]\n",
      "Episode 910\tLast Epsilon: 0.40\n",
      "Elapsed time 221.5 Steps 920000 Agent Steps 920001\n",
      "Episode 920\tAverage Score: 38.00\n",
      "Episode 920\tLast Action: [-0.13329224  0.9865709  -0.34138262 -0.22455244]\n",
      "Episode 920\tLast Epsilon: 0.40\n",
      "Elapsed time 223.8 Steps 930000 Agent Steps 930001\n",
      "Episode 930\tAverage Score: 37.08\n",
      "Episode 930\tLast Action: [ 0.924192   -0.80117875 -0.23064037  1.        ]\n",
      "Episode 930\tLast Epsilon: 0.39\n",
      "Elapsed time 226.3 Steps 940000 Agent Steps 940001\n",
      "Episode 940\tAverage Score: 36.99\n",
      "Episode 940\tLast Action: [-0.8625635  -0.01503513 -0.01559252 -0.1298235 ]\n",
      "Episode 940\tLast Epsilon: 0.39\n",
      "Elapsed time 228.7 Steps 950000 Agent Steps 950001\n",
      "Episode 950\tAverage Score: 36.89\n",
      "Episode 950\tLast Action: [-1.         -0.408809   -0.3431005  -0.49661547]\n",
      "Episode 950\tLast Epsilon: 0.39\n",
      "Elapsed time 231.0 Steps 960000 Agent Steps 960001\n",
      "Episode 960\tAverage Score: 34.03\n",
      "Episode 960\tLast Action: [-0.7144711  -0.5466425  -0.394447   -0.01330667]\n",
      "Episode 960\tLast Epsilon: 0.38\n",
      "Elapsed time 233.4 Steps 970000 Agent Steps 970001\n",
      "Episode 970\tAverage Score: 34.57\n",
      "Episode 970\tLast Action: [-0.44957182 -0.83791244  0.92364544 -0.14761847]\n",
      "Episode 970\tLast Epsilon: 0.38\n",
      "Elapsed time 235.8 Steps 980000 Agent Steps 980001\n",
      "Episode 980\tAverage Score: 36.38\n",
      "Episode 980\tLast Action: [-0.6483082   1.          0.02229179 -0.0552489 ]\n",
      "Episode 980\tLast Epsilon: 0.38\n",
      "Elapsed time 238.2 Steps 990000 Agent Steps 990001\n",
      "Episode 990\tAverage Score: 34.31\n",
      "Episode 990\tLast Action: [-0.956188    1.         -0.14400917 -0.12135345]\n",
      "Episode 990\tLast Epsilon: 0.37\n",
      "Elapsed time 240.6 Steps 1000000 Agent Steps 1000001\n",
      "Episode 1000\tAverage Score: 36.21\n",
      "Episode 1000\tLast Action: [ 0.99721754  0.76277286 -0.04948714  0.9138846 ]\n",
      "Episode 1000\tLast Epsilon: 0.37"
     ]
    }
   ],
   "source": [
    "scores = Interact(env, agent, brain_name, num_agents, train_mode=True, add_noise=True, n_episodes=1000,\n",
    "                  max_t=1000, eps_start=1, eps_end=0.2, eps_decay=0.999, n_window=10, tgt_score=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(scores).to_pickle(\"./Data/scores.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores_win = scores[0:200]\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores_win)), scores_win)\n",
    "plt.axhline(y=30, color='r', linestyle='dashed')\n",
    "\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n",
    "fig.savefig('./Plots/Average_Score.pdf')\n",
    "fig.savefig('./Plots/Average_Score.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLAY SMART AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('./Data/checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('./Data/checkpoint_critic.pth'))\n",
    "\n",
    "## Play \n",
    "for i in range(3):\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "    scores = np.zeros(num_agents)\n",
    "    for j in range(200):\n",
    "        actions = agent.act(states)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        next_states = env_info.vector_observations    # get the next state\n",
    "        rewards = env_info.rewards                    # get the reward\n",
    "        dones = env_info.local_done                   # see if episode has finished\n",
    "        scores += rewards                             # update the score\n",
    "        states = next_states                          # roll over the state to next time step\n",
    "        if np.any(dones):\n",
    "            break\n",
    "\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
